{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Athena using standar SQL syntax\n",
    "Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.\n",
    "\n",
    "Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically—running queries in parallel—so results are fast, even with large datasets and complex queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed modules\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify AWS Credentials\n",
    "session = boto3.Session()\n",
    "sts = session.client('sts')\n",
    "response = sts.get_caller_identity()\n",
    "my_username = response['Arn'].split('/')[1]\n",
    "print(my_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use 3 AWS services: glue, athena and s3, so create their clients\n",
    "glue = boto3.client('glue')\n",
    "athena = boto3.client('athena')\n",
    "s3c = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query an Athena table step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the right database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = glue.get_databases()\n",
    "databases\n",
    "db_lst = []\n",
    "for i in databases['DatabaseList']:\n",
    "    #print(i['Name'])\n",
    "    db_lst.append(i['Name'])\n",
    "print('Existing databases in Athena:',db_lst)\n",
    "# Our data is in the 'world-bank-indicators', in index [2] from the list. Let's store that value in a variable\n",
    "db = db_lst[2]\n",
    "print('Our selected database:',db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what tables are available in the database\n",
    "I have already used an AWS Glue crawler to search the files and find any common structures and create tables. This resulted in two tables:\n",
    "- gdp: The World Bank indicator, GDP, which has a row for each country's gross domestic product in the years 2060-2020\n",
    "- pop: The World Bank indicator, POP, which has a row for each country's population in the years 2060-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = glue.get_tables(DatabaseName = db)\n",
    "# Print out all the tabes in this database\n",
    "for t in tables['TableList']:\n",
    "    print(t['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters needed to connect to Athena and run a SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to have an S3 location to store our query results. Set it. This is just a string variable.\n",
    "output = 's3://gse580/athena/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and run the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query using Standard SQL syntax. It needs to be a string.\n",
    "query = 'SELECT * FROM pop WHERE value < 1000000 and date = 2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query using the start_query_execution() function. \n",
    "athenaQuery = athena.start_query_execution(\n",
    "    # Specify the query\n",
    "    QueryString = query,\n",
    "    # Specify the database\n",
    "    QueryExecutionContext = {\n",
    "        'Database': db\n",
    "    }, \n",
    "    # Send the results to the output location on S3\n",
    "    ResultConfiguration = { 'OutputLocation': output}\n",
    ")\n",
    "# Athena reports back details about this query. As usual, this is a dictionary (or JSON object)\n",
    "athenaQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the QueryExecutionId from the athenaQuery variable\n",
    "# An important item to know is the id of the query\n",
    "qid = athenaQuery['QueryExecutionId']\n",
    "print('Query ID:',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query may take some time, so make sure it has 'SUCCEEDED' before moving on\n",
    "#\n",
    "# Request the query's current status using the get_query_execution() function.\n",
    "response = athena.get_query_execution(QueryExecutionId=qid)\n",
    "# Print the current status\n",
    "response['QueryExecution']['Status']['State']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the results of a query into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Athena stored the result of the query in a .csv file in our S3 output location: 's3://gse580/athena/results'\n",
    "bucket = 'gse580'\n",
    "# Parse the reqponse variable to get just the key to the file\n",
    "key = response['QueryExecution']['ResultConfiguration']['OutputLocation'].split('gse580/')[1]\n",
    "print('Bucket:',bucket)\n",
    "print('Key:',key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the 'get_object' function from boto3. We did this earlier in the term.\n",
    "response = s3c.get_object(Bucket=bucket, Key=key)\n",
    "#\n",
    "# Get the HTTPStatusCode from the response\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    # If all OK, then create the DataFrame\n",
    "    print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "    # Create the df from the get_object call above by getting only the 'Body' (which is the data)\n",
    "    df = pd.read_csv(response.get('Body'))\n",
    "else:\n",
    "    # See what the response is and troubleshoot\n",
    "    print(f\"Unsuccessful S3 get_object response. Status - {status}\")\n",
    "#\n",
    "# Assuming it worked, show the df.head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While that was a lot of steps, we can write a Python function to do it all for us.\n",
    "Let's setup the function to work like this:\n",
    "1. Define a query string\n",
    "2. Call the function with the query as a parameter and return a pandas DataFrame\n",
    "\n",
    "You will call the function like this:\n",
    "- query = 'Write a SQL query here\"\n",
    "- df = query_athena(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample query for a test\n",
    "query = \"SELECT * FROM gdp WHERE countryiso3code = 'USA'\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes the query as a parameter and returns a DataFrame.\n",
    "# This is the same code used above, just included in one function\n",
    "def query_athena(query):\n",
    "    # Define variables that are needed\n",
    "    db = 'wb-indicators'\n",
    "    output = 's3://gse580/athena/results'\n",
    "    bucket = 'gse580'\n",
    "    # Start the query\n",
    "    athenaQuery = athena.start_query_execution(\n",
    "        QueryString = query,\n",
    "        QueryExecutionContext = {\n",
    "            'Database': db\n",
    "        }, \n",
    "        ResultConfiguration = { 'OutputLocation': output}\n",
    "    )\n",
    "    while True:\n",
    "        # Wait for a second for query to complete.\n",
    "        time.sleep(1)\n",
    "        qid = athenaQuery['QueryExecutionId']\n",
    "        try:\n",
    "            # Check status\n",
    "            response = athena.get_query_execution(QueryExecutionId = qid)\n",
    "            # Test to see if query was successful\n",
    "            if response['QueryExecution']['Status']['State'] == 'SUCCEEDED':\n",
    "                print('Successful query')\n",
    "                break\n",
    "            else:\n",
    "                # Print the status\n",
    "                print('Still waiting for the query:',response['QueryExecution']['Status']['State'])\n",
    "                # If the state is 'FAILED', then exit while loop. \n",
    "                if response['QueryExecution']['Status']['State'] == 'FAILED':\n",
    "                    print('Failed query, investigate error')\n",
    "                    break\n",
    "        except:\n",
    "            print('Query not yet done, waiting for a second')\n",
    "    # Parse the query reqponse to get just the key to the file\n",
    "    key = response['QueryExecution']['ResultConfiguration']['OutputLocation'].split('gse580/')[1]\n",
    "    #\n",
    "    # Call the S3 'get_object' function from boto3.\n",
    "    response = s3c.get_object(Bucket=bucket, Key=key)\n",
    "    #\n",
    "    # Get the HTTPStatusCode from the response\n",
    "    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "    if status == 200:\n",
    "        # If all OK, then create the DataFrame\n",
    "        #print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "        # Create the df from the get_object call above by getting only the 'Body' (which is the data)\n",
    "        df = pd.read_csv(response.get('Body'))\n",
    "    else:\n",
    "        # See what the response is and troubleshoot\n",
    "        print(f\"Unsuccessful S3 get_object response. Status - {status}\")\n",
    "    #\n",
    "    # Assuming it worked, return the df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now just call the query function and load the results into a variable.\n",
    "query_df = query_athena(query)\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's combine the two tables into a single table with both POP and GDP for all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get everything from the population table with useful column names\n",
    "query = \"SELECT pop.countryiso3code AS country,pop.date,pop.value AS pop FROM pop\"\n",
    "print(query)\n",
    "pop_df = query_athena(query)\n",
    "print(pop_df.head(2))\n",
    "# Show the dimension of the dataframe\n",
    "pop_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get everything from the gdp table with useful column names\n",
    "query = \"SELECT gdp.countryiso3code AS country,gdp.date,gdp.value AS gdp FROM gdp\"\n",
    "print(query)\n",
    "gdp_df = query_athena(query)\n",
    "print(gdp_df.head(2))\n",
    "# Show the dimension\n",
    "gdp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, the data is not very clean, lots of NaNs.\n",
    "pop_df[pop_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean it up\n",
    "# Drop any NaNs\n",
    "pop_df = pop_df.dropna()\n",
    "gdp_df = gdp_df.dropna()\n",
    "# Let's reset the DataFrames index so they are increasing 1,2,3...\n",
    "pop_df.reset_index()\n",
    "gdp_df.reset_index()\n",
    "# See the size of each df\n",
    "print(pop_df.shape)\n",
    "print(gdp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 2 dfs into single df\n",
    "merged_df = pop_df.merge(gdp_df, on=['country','date'])\n",
    "print(merged_df.head(5))\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment:\n",
    "- Create your own, different query of the tables, creating a new dataframe.  The query can be whatever you want.\n",
    "- Save that dataframe to a new .csv on the 'gse580' bucket at the location:<BR>\n",
    "        /gsb580/yourusername/data/athena_results.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
